"""
Tab de EvaluaciÃ³n de Modelos.
"""
import streamlit as st
from typing import Dict, Any, List
import pandas as pd

from src.evaluation import ModelEvaluator, evaluate_classifiers_with_split
from src.visualization import (
    plot_metrics_comparison, 
    plot_scores_distribution,
    plot_genre_diversity,
    plot_overlap_comparison,
    plot_classifier_comparison,
    plot_training_times
)
from src.config import get_model_color


def _render_metrics_help():
    """Renderiza el panel de ayuda de mÃ©tricas."""
    with st.expander("ğŸ“š **GuÃ­a de MÃ©tricas** - Â¿QuÃ© significa cada una?", expanded=False):
        st.markdown("""
        ### ğŸ¯ MÃ©tricas de RecomendaciÃ³n
        
        | MÃ©trica | Â¿QuÃ© mide? | FÃ³rmula | Ejemplo |
        |---------|-----------|---------|---------|
        | **Precision@K** | De las K recomendaciones, Â¿cuÃ¡ntas son buenas? | `Relevantes en K / K` | K=5, 3 buenas â†’ **60%** |
        | **Recall@K** | De TODAS las buenas, Â¿cuÃ¡ntas capturamos en K? | `Relevantes en K / Total Relevantes` | 20 relevantes, 4 en K=5 â†’ **20%** |
        | **nDCG@K** | Â¿Los buenos estÃ¡n arriba en el ranking? | Penaliza posiciones bajas | Relevante en #1 > en #5 |
        | **MAP** | PrecisiÃ³n promedio en cada "acierto" | Combina orden y cantidad | Calidad general del ranking |
        | **Genre Diversity** | Â¿QuÃ© tan variadas son las recomendaciones? | `GÃ©neros Ãºnicos / Total` | Alta = mÃ¡s variedad |
        
        ---
        
        ### âš–ï¸ Trade-offs Importantes
        
        ```
        K pequeÃ±o (3-5)          vs          K grande (15-20)
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        âœ… Alta Precision                 âœ… Alto Recall
        âŒ Bajo Recall                    âŒ Baja Precision
        ğŸ¯ MÃ¡s exigente                   ğŸ¯ MÃ¡s permisivo
        ```
        
        ---
        
        ### ğŸ² Â¿QuÃ© hace cada parÃ¡metro?
        
        | ParÃ¡metro | Efecto al AUMENTAR | Efecto al DISMINUIR |
        |-----------|-------------------|---------------------|
        | **Top K** | â†‘ Recall, â†“ Precision | â†“ Recall, â†‘ Precision |
        | **PelÃ­culas de prueba** | MÃ¡s lento pero mÃ¡s confiable | MÃ¡s rÃ¡pido pero ruidoso |
        | **Semilla** | Controla quÃ© pelÃ­culas se prueban (reproducibilidad) | Diferente seed = diferentes pelÃ­culas |
        
        ---
        
        ### ğŸ’¡ ConfiguraciÃ³n Recomendada
        
        | Escenario | K | PelÃ­culas | Por quÃ© |
        |-----------|---|-----------|---------|
        | Demo rÃ¡pida | 5 | 20 | Resultados instantÃ¡neos |
        | EvaluaciÃ³n seria | 10 | 50+ | Balance velocidad/precisiÃ³n |
        | ComparaciÃ³n final | 10 | 100 | Resultados estables |
        """)


def _render_parameters_help():
    """Renderiza ayuda sobre los parÃ¡metros."""
    with st.expander("âš™ï¸ **Â¿CÃ³mo afectan los parÃ¡metros?**", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            #### ğŸ”¢ Top K
            
            **K = 5 (pocos resultados)**
            - Precision alta (fÃ¡cil acertar con pocos)
            - Recall bajo (menos oportunidades)
            - MÃ¡s exigente con el modelo
            
            **K = 20 (muchos resultados)**
            - Precision baja (difÃ­cil que todos sean buenos)
            - Recall alto (mÃ¡s oportunidades de encontrar)
            - MÃ¡s permisivo
            """)
        
        with col2:
            st.markdown("""
            #### ğŸ² Semilla Aleatoria
            
            **Â¿Por quÃ© importa?**
            - Controla QUÃ‰ pelÃ­culas se usan para evaluar
            - Mismo valor = resultados reproducibles
            - Diferente valor = diferentes pelÃ­culas de prueba
            
            **Tip:** Prueba varias semillas (42, 123, 456) 
            y promedia para resultados mÃ¡s robustos.
            """)


def render_tab_evaluation(models: Dict, data: pd.DataFrame, benchmark: Dict):
    """
    Renderiza el tab de evaluaciÃ³n de modelos.
    
    Args:
        models: Diccionario de modelos
        data: DataFrame con datos
        benchmark: Datos del benchmark
    """
    st.markdown("### ğŸ“Š EvaluaciÃ³n de Modelos")
    
    # Panel de ayuda principal
    _render_metrics_help()
    
    # Sub-tabs para diferentes tipos de evaluaciÃ³n
    eval_tab1, eval_tab2, eval_tab3 = st.tabs([
        "ğŸ¯ Recomendadores", 
        "ğŸ·ï¸ Clasificadores",
        "â±ï¸ Benchmark"
    ])
    
    with eval_tab1:
        _render_recommender_evaluation(models, data)
    
    with eval_tab2:
        _render_classifier_evaluation(data)
    
    with eval_tab3:
        _render_benchmark(benchmark)


def _render_recommender_evaluation(models: Dict, data: pd.DataFrame):
    """EvaluaciÃ³n de recomendadores."""
    
    st.markdown("""
    <div style="background: #1f1f1f; padding: 1rem; border-radius: 10px; border-left: 4px solid #e50914; margin-bottom: 1rem;">
        <p style="color: #b3b3b3; margin: 0;">
        <b>MetodologÃ­a:</b> Usamos <b>gÃ©neros como ground truth</b>. Si una recomendaciÃ³n comparte 
        al menos un gÃ©nero con la pelÃ­cula original, la consideramos "relevante".
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Panel de ayuda de parÃ¡metros
    _render_parameters_help()
    
    # ConfiguraciÃ³n
    st.markdown("#### âš™ï¸ ConfiguraciÃ³n del Experimento")
    col1, col2, col3 = st.columns(3)
    with col1:
        n_test = st.slider(
            "ğŸ¬ PelÃ­culas de prueba:", 
            10, 100, 30, 
            key="eval_n_test",
            help="MÃ¡s pelÃ­culas = resultados mÃ¡s estables pero mÃ¡s lento"
        )
    with col2:
        k_value = st.slider(
            "ğŸ”¢ Top K:", 
            3, 20, 5, 
            key="eval_k",
            help="K bajo = mÃ¡s exigente (alta precision). K alto = mÃ¡s permisivo (alto recall)"
        )
    with col3:
        seed = st.number_input(
            "ğŸ² Semilla:", 
            0, 1000, 42, 
            key="eval_seed",
            help="Mismo valor = mismas pelÃ­culas de prueba (reproducible)"
        )
    
    if st.button("ğŸ”¬ Calcular MÃ©tricas", type="primary", key="run_rec_eval"):
        if models.get("recommenders") and data is not None:
            evaluator = ModelEvaluator(data)
            
            progress = st.progress(0)
            status = st.empty()
            
            def update_progress(p, text):
                progress.progress(p)
                status.text(text)
            
            results = evaluator.evaluate_all_recommenders(
                models["recommenders"],
                n_test=n_test,
                k=k_value,
                seed=seed,
                progress_callback=update_progress
            )
            
            progress.empty()
            status.empty()
            
            if results:
                st.session_state['rec_eval_results'] = results
                st.success(f"âœ… Evaluados {len(results)} modelos")
    
    # Mostrar resultados
    if 'rec_eval_results' in st.session_state:
        results = st.session_state['rec_eval_results']
        
        # Tabla
        df = pd.DataFrame(results).T.round(4)
        st.dataframe(df, use_container_width=True)
        
        # GrÃ¡fico
        fig = plot_metrics_comparison(results)
        st.plotly_chart(fig, use_container_width=True)
        
        # Mejor por mÃ©trica
        st.markdown("#### ğŸ† Mejor Modelo por MÃ©trica")
        metrics = ["Precision@K", "Recall@K", "nDCG@K", "MAP"]
        cols = st.columns(len(metrics))
        
        for col, metric in zip(cols, metrics):
            with col:
                best = max(results.items(), key=lambda x: x[1].get(metric, 0))
                color = get_model_color(best[0])
                st.markdown(f"""
                <div style="background: {color}22; border: 2px solid {color}; border-radius: 8px; padding: 0.8rem; text-align: center;">
                    <small style="color: #888;">{metric}</small><br>
                    <b style="color: white;">{best[0]}</b><br>
                    <span style="color: {color};">{best[1].get(metric, 0):.4f}</span>
                </div>
                """, unsafe_allow_html=True)


def _render_classifier_evaluation(data: pd.DataFrame):
    """EvaluaciÃ³n de clasificadores."""
    
    st.markdown("""
    Entrenamos clasificadores con train/test split para obtener mÃ©tricas reales.
    """)
    
    col1, col2 = st.columns(2)
    with col1:
        test_size = st.slider("% Test:", 10, 40, 20, key="clf_test_size")
    with col2:
        sample_size = st.slider("Muestra:", 500, 5000, 2000, key="clf_sample")
    
    if st.button("ğŸ”¬ Evaluar Clasificadores", type="primary", key="run_clf_eval"):
        if data is not None:
            progress = st.progress(0)
            status = st.empty()
            
            def update_progress(p, text):
                progress.progress(p)
                status.text(text)
            
            results = evaluate_classifiers_with_split(
                data,
                test_size=test_size/100,
                sample_size=sample_size,
                progress_callback=update_progress
            )
            
            progress.empty()
            status.empty()
            
            if results:
                st.session_state['clf_eval_results'] = results
                st.success(f"âœ… Evaluados {len(results)} clasificadores")
    
    # Mostrar resultados
    if 'clf_eval_results' in st.session_state:
        results = st.session_state['clf_eval_results']
        
        # Cards de resumen
        cols = st.columns(len(results))
        colors = {"Logistic Regression": "#FF9800", "Naive Bayes": "#9C27B0", "Random Forest": "#4CAF50"}
        
        for col, (name, metrics) in zip(cols, results.items()):
            with col:
                color = colors.get(name, "#666")
                st.markdown(f"""
                <div style="background: {color}22; border: 2px solid {color}; border-radius: 10px; padding: 1rem; text-align: center;">
                    <b style="color: {color};">{name.split()[0]}</b><br>
                    <span style="color: white; font-size: 1.5rem;">{metrics['f1_micro']:.1%}</span><br>
                    <small style="color: #888;">F1 Micro</small>
                </div>
                """, unsafe_allow_html=True)
        
        # GrÃ¡fico
        fig = plot_classifier_comparison(results)
        st.plotly_chart(fig, use_container_width=True)
        
        # Tabla completa
        with st.expander("ğŸ“‹ Tabla completa"):
            df_data = []
            for name, m in results.items():
                df_data.append({
                    "Clasificador": name,
                    "F1 Micro": f"{m['f1_micro']:.2%}",
                    "F1 Macro": f"{m['f1_macro']:.2%}",
                    "Precision": f"{m['precision']:.2%}",
                    "Recall": f"{m['recall']:.2%}",
                    "Hamming Loss": f"{m['hamming_loss']:.4f}",
                    "Tiempo (s)": f"{m['train_time']:.2f}"
                })
            st.dataframe(pd.DataFrame(df_data), use_container_width=True, hide_index=True)


def _render_benchmark(benchmark: Dict):
    """Muestra datos del benchmark."""
    
    info = benchmark.get("dataset_info", {})
    
    # MÃ©tricas del dataset
    st.markdown("#### ğŸ“Š Dataset")
    cols = st.columns(4)
    
    with cols[0]:
        st.metric("TÃ­tulos", f"{info.get('total_titles', 0):,}")
    with cols[1]:
        st.metric("GÃ©neros", info.get('unique_genres', 0))
    with cols[2]:
        st.metric("PelÃ­culas", info.get('movies', 0))
    with cols[3]:
        st.metric("Series", info.get('tv_shows', 0))
    
    # Tiempos de entrenamiento
    st.markdown("#### â±ï¸ Tiempos de Entrenamiento")
    
    fig = plot_training_times(benchmark)
    st.plotly_chart(fig, use_container_width=True)
    
    # Tabla comparativa
    st.markdown("#### ğŸ“‹ Comparativa TÃ©cnica")
    
    st.markdown("""
    | CaracterÃ­stica | TF-IDF | Doc2Vec | SBERT |
    |---------------|--------|---------|-------|
    | âš¡ Velocidad | â­â­â­â­â­ | â­â­â­ | â­â­ |
    | ğŸ¯ PrecisiÃ³n | â­â­ | â­â­â­ | â­â­â­â­â­ |
    | ğŸ’¾ Memoria | Bajo | Medio | Alto |
    | ğŸ”¤ SinÃ³nimos | âŒ | âš¡ Parcial | âœ… |
    | ğŸŒ Pre-entrenado | âŒ | âŒ | âœ… |
    """)
    
    if benchmark.get("generated_at"):
        st.caption(f"ğŸ“… Generado: {benchmark['generated_at'][:19]}")
